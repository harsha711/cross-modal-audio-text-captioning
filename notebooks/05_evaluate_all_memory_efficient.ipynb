{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate and Compare All Models (Memory-Efficient Versions)\n",
        "\n",
        "This notebook loads all **memory-efficient** trained models and compares their performance.\n",
        "\n",
        "**Models**: Baseline (small), Attention (small), Transformer (small)\n",
        "\n",
        "**Comparison metrics**:\n",
        "- Validation loss\n",
        "- Repetition rate\n",
        "- Vocabulary diversity\n",
        "- Caption length\n",
        "- Sample predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path('..').absolute()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from src.models import create_model\n",
        "from src.dataset import ClothoEvalDataset\n",
        "from src.evaluation import compare_models, get_sample_predictions, print_sample_predictions\n",
        "from src.utils import load_vocab, get_device, plot_evaluation_metrics\n",
        "\n",
        "print(\"✓ Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup Device and Load Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    free_memory = torch.cuda.mem_get_info()[0] / 1024**3\n",
        "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
        "    print(f\"Free GPU memory: {free_memory:.2f} GB\")\n",
        "\n",
        "vocab = load_vocab('../vocab.json')\n",
        "print(f\"\\n✓ Vocabulary size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Evaluation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading evaluation dataset...\")\n",
        "eval_dataset = ClothoEvalDataset(\n",
        "    captions_file='../data/eval_captions.json',\n",
        "    features_dir='../features/mel_eval/',\n",
        "    vocab=vocab\n",
        ")\n",
        "print(f\"✓ Loaded {len(eval_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load All Memory-Efficient Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading memory-efficient trained models...\\n\")\n",
        "models_dict = {}\n",
        "\n",
        "# Model configurations (memory-efficient versions)\n",
        "model_configs = {\n",
        "    'baseline_small': {\n",
        "        'type': 'baseline',\n",
        "        'params': {\n",
        "            'vocab_size': len(vocab),\n",
        "            'embed_dim': 128,\n",
        "            'hidden_dim': 256,\n",
        "            'num_layers': 1\n",
        "        }\n",
        "    },\n",
        "    'attention_small': {\n",
        "        'type': 'attention',\n",
        "        'params': {\n",
        "            'vocab_size': len(vocab),\n",
        "            'embed_dim': 128,\n",
        "            'hidden_dim': 256,\n",
        "            'num_layers': 1\n",
        "        }\n",
        "    },\n",
        "    'transformer_small': {\n",
        "        'type': 'transformer',\n",
        "        'params': {\n",
        "            'vocab_size': len(vocab),\n",
        "            'd_model': 256,\n",
        "            'nhead': 4,\n",
        "            'num_encoder_layers': 2,\n",
        "            'num_decoder_layers': 2,\n",
        "            'dim_feedforward': 512\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "for model_name, config in model_configs.items():\n",
        "    checkpoint_path = Path(f'../checkpoints/best_{model_name}.pth')\n",
        "    \n",
        "    if checkpoint_path.exists():\n",
        "        print(f\"Loading {model_name}...\")\n",
        "        \n",
        "        # Create model with specific configuration\n",
        "        model = create_model(config['type'], **config['params'])\n",
        "        \n",
        "        # Load checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        models_dict[model_name] = model\n",
        "        print(f\"  ✓ {model_name} loaded\")\n",
        "    else:\n",
        "        print(f\"  ⚠ Checkpoint not found: {checkpoint_path}\")\n",
        "\n",
        "print(f\"\\n✓ Loaded {len(models_dict)} memory-efficient models\")\n",
        "\n",
        "# Check GPU memory after loading all models\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    free = torch.cuda.mem_get_info()[0] / 1024**3\n",
        "    print(f\"\\nGPU Memory Status:\")\n",
        "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"  Free: {free:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Compare All Models\n",
        "\n",
        "This will evaluate each model on the evaluation set and compare metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"COMPARING ALL MEMORY-EFFICIENT MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison = compare_models(\n",
        "    models_dict=models_dict,\n",
        "    eval_dataset=eval_dataset,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    num_samples=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Comparison Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comparison results\n",
        "results_to_save = {}\n",
        "for model_name, data in comparison.items():\n",
        "    results_to_save[model_name] = data['metrics']\n",
        "\n",
        "with open('../results/comparison_results_small.json', 'w') as f:\n",
        "    json.dump(results_to_save, f, indent=2)\n",
        "\n",
        "print(\"✓ Comparison results saved to results/comparison_results_small.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Plot Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating comparison plots...\")\n",
        "plot_evaluation_metrics(comparison)\n",
        "plt.savefig('../results/comparison_plot_small.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Plot saved to results/comparison_plot_small.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Sample Predictions from Each Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for model_name, model in models_dict.items():\n",
        "    print(f\"\\n{model_name.upper().replace('_', ' ')}:\")\n",
        "    print(\"-\"*80)\n",
        "    samples = get_sample_predictions(model, eval_dataset, vocab, device, num_samples=5)\n",
        "    print_sample_predictions(samples, num_to_print=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Load Training Histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training histories\n",
        "histories = {}\n",
        "for model_name in models_dict.keys():\n",
        "    history_path = Path(f'../results/{model_name}_history.json')\n",
        "    if history_path.exists():\n",
        "        with open(history_path, 'r') as f:\n",
        "            histories[model_name] = json.load(f)\n",
        "\n",
        "print(f\"✓ Loaded {len(histories)} training histories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Plot Training Curves Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import plot_model_comparison\n",
        "\n",
        "# Compare validation losses\n",
        "if len(histories) > 0:\n",
        "    print(\"Plotting training curves comparison...\")\n",
        "    plot_model_comparison(histories, metric='val_loss')\n",
        "    plt.savefig('../results/training_comparison_small.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"✓ Training comparison saved to results/training_comparison_small.png\")\n",
        "else:\n",
        "    print(\"⚠ No training histories found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary dataframe\n",
        "summary_data = []\n",
        "for model_name in models_dict.keys():\n",
        "    metrics = comparison[model_name]['metrics']\n",
        "    history = histories.get(model_name, {})\n",
        "    \n",
        "    # Clean up model name for display\n",
        "    display_name = model_name.replace('_small', '').replace('_', ' ').title()\n",
        "    \n",
        "    summary_data.append({\n",
        "        'Model': display_name + ' (Small)',\n",
        "        'Best Val Loss': min(history.get('val_loss', [float('inf')])),\n",
        "        'Repetition Rate': metrics['avg_repetition_rate'],\n",
        "        'Vocabulary Diversity': metrics['vocabulary_diversity'],\n",
        "        'Avg Caption Length': metrics['mean_caption_length'],\n",
        "        'Unique Words Used': metrics['unique_words_used']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON SUMMARY (Memory-Efficient Versions)\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save to CSV\n",
        "summary_df.to_csv('../results/model_comparison_small.csv', index=False)\n",
        "print(\"\\n✓ Summary saved to results/model_comparison_small.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Best Model Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model based on validation loss\n",
        "best_model = min(summary_data, key=lambda x: x['Best Val Loss'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST MODEL (Among Memory-Efficient Versions)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nModel: {best_model['Model']}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "for key, value in best_model.items():\n",
        "    if key != 'Model':\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All memory-efficient models have been evaluated and compared!\n",
        "\n",
        "**Files created:**\n",
        "- `results/comparison_results_small.json` - Detailed metrics for all models\n",
        "- `results/comparison_plot_small.png` - Visual comparison\n",
        "- `results/training_comparison_small.png` - Training curves comparison\n",
        "- `results/model_comparison_small.csv` - Summary table\n",
        "\n",
        "**Memory-efficient configurations used:**\n",
        "- Baseline: embed_dim=128, hidden_dim=256, num_layers=1\n",
        "- Attention: embed_dim=128, hidden_dim=256, num_layers=1\n",
        "- Transformer: d_model=256, nhead=4, layers=2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
