{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Transformer Model (Memory-Efficient for 8GB GPU)\n",
        "\n",
        "This notebook trains a **memory-optimized** transformer model.\n",
        "\n",
        "**Optimizations for 8GB GPU**:\n",
        "- Smaller d_model (256 vs 512)\n",
        "- Fewer attention heads (4 vs 8)\n",
        "- Fewer encoder/decoder layers (2 vs 3)\n",
        "- Smaller batch size (8 vs 32)\n",
        "\n",
        "**Model**: CNN + Transformer encoder-decoder with positional encoding\n",
        "\n",
        "**Estimated time**: 4-6 hours (40 epochs on GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['MKL_THREADING_LAYER'] = 'GNU'  # Fix threading issue\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Better memory allocation\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path\n",
        "project_root = Path('..').absolute()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "import json\n",
        "from src.models import create_model\n",
        "from src.dataset import create_dataloaders\n",
        "from src.trainer import ModelTrainer\n",
        "from src.utils import load_vocab, set_seed, get_device, count_parameters, make_json_serializable\n",
        "\n",
        "\n",
        "print(\"✓ Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Random Seed and Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "device = get_device()\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    free_memory = torch.cuda.mem_get_info()[0] / 1024**3\n",
        "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
        "    print(f\"Free GPU memory: {free_memory:.2f} GB\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No GPU available, using CPU (will be slower)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading vocabulary...\")\n",
        "vocab = load_vocab('../vocab.json')\n",
        "print(f\"\\n✓ Vocabulary size: {len(vocab)}\")\n",
        "print(f\"  <pad>: {vocab['<pad>']}\")\n",
        "print(f\"  <sos>: {vocab['<sos>']}\")\n",
        "print(f\"  <eos>: {vocab['<eos>']}\")\n",
        "print(f\"  <unk>: {vocab['<unk>']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Dataloaders with Small Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating dataloaders...\")\n",
        "print(\"Using batch_size=8 (transformers use most memory!)\\n\")\n",
        "\n",
        "train_loader, val_loader, eval_dataset = create_dataloaders(\n",
        "    train_captions='../data/train_captions.json',\n",
        "    val_captions='../data/val_captions.json',\n",
        "    eval_captions='../data/eval_captions.json',\n",
        "    train_features_dir='../features/mel/',\n",
        "    val_features_dir='../features/mel/',\n",
        "    eval_features_dir='../features/mel_eval/',\n",
        "    vocab=vocab,\n",
        "    batch_size=8,      # Smallest batch size - transformers are memory hungry\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Dataloaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Eval samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Memory-Efficient Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating MEMORY-EFFICIENT transformer model...\\n\")\n",
        "print(\"Model configuration (optimized for 8GB GPU):\")\n",
        "print(\"  - d_model: 256 (reduced from 512)\")\n",
        "print(\"  - num_heads: 4 (reduced from 8)\")\n",
        "print(\"  - num_encoder_layers: 2 (reduced from 3)\")\n",
        "print(\"  - num_decoder_layers: 2 (reduced from 3)\")\n",
        "print(\"  - dim_feedforward: 512 (reduced from 2048)\")\n",
        "print(\"\\nThis significantly reduces memory usage while maintaining transformer architecture\\n\")\n",
        "\n",
        "model = create_model(\n",
        "    'transformer', \n",
        "    vocab_size=len(vocab),\n",
        "    d_model=256,              # Reduced from 512\n",
        "    nhead=4,                  # Reduced from 8\n",
        "    num_encoder_layers=2,     # Reduced from 3\n",
        "    num_decoder_layers=2,     # Reduced from 3\n",
        "    dim_feedforward=512       # Reduced from 2048\n",
        ")\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "\n",
        "print(\"\\nParameter count:\")\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "\n",
        "# Estimate memory\n",
        "model_memory = total_params * 4 / (1024**3)  # 4 bytes per param (float32)\n",
        "print(f\"\\nEstimated model memory: {model_memory:.2f} GB\")\n",
        "\n",
        "# Move to GPU and check actual usage\n",
        "print(\"\\nMoving model to GPU...\")\n",
        "model = model.to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    free = torch.cuda.mem_get_info()[0] / 1024**3\n",
        "    \n",
        "    print(f\"\\nGPU Memory Status After Loading Model:\")\n",
        "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
        "    print(f\"  Free: {free:.2f} GB\")\n",
        "    \n",
        "    if free > 3:\n",
        "        print(f\"\\n✓ Model fits comfortably in GPU memory! ({free:.2f} GB free)\")\n",
        "    elif free > 1:\n",
        "        print(f\"\\n✓ Model fits in GPU memory! ({free:.2f} GB free)\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️ Low memory warning: only {free:.2f} GB free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Create Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Initializing trainer...\")\n",
        "\n",
        "trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    model_name='transformer_small'  # Different name to distinguish from full-size model\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Train Model\n",
        "\n",
        "**This will take 4-6 hours on GPU**\n",
        "\n",
        "Progress will be shown with:\n",
        "- Training loss per batch (progress bar)\n",
        "- Validation loss per epoch\n",
        "- Sample generations every 5 epochs\n",
        "- Learning rate changes\n",
        "- Early stopping if no improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\\n\")\n",
        "print(\"=\"*80)\n",
        "print(\"TRANSFORMER MODEL TRAINING (Memory-Efficient Version)\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nTraining smaller transformer model optimized for 8GB GPU\")\n",
        "print(\"Expected training time: 4-6 hours\")\n",
        "print(\"\\nMonitor GPU memory during training:\")\n",
        "print(\"  watch -n 1 nvidia-smi\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "history = trainer.fit(\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    eval_dataset=eval_dataset,\n",
        "    num_epochs=40,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    patience=5,\n",
        "    label_smoothing=0.1,\n",
        "    save_dir='../checkpoints'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import plot_training_history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Plotting training history...\")\n",
        "plot_training_history(history)\n",
        "plt.show()\n",
        "\n",
        "# Save history\n",
        "with open('../results/transformer_small_history.json', 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "print(\"\\n✓ History saved to results/transformer_small_history.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.evaluation import evaluate_model\n",
        "\n",
        "print(\"Running final evaluation...\\n\")\n",
        "\n",
        "results, captions, refs = evaluate_model(\n",
        "    trainer.model,\n",
        "    eval_dataset,\n",
        "    vocab,\n",
        "    device=device,\n",
        "    num_samples=100\n",
        ")\n",
        "\n",
        "# Save results\n",
        "serializable_results = make_json_serializable(results)\n",
        "with open('../results/transformer_small_results.json', 'w') as f:\n",
        "    json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "\n",
        "print(\"\\n✓ Results saved to results/transformer_small_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Show Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.evaluation import get_sample_predictions, print_sample_predictions\n",
        "\n",
        "print(\"Generating sample predictions...\\n\")\n",
        "\n",
        "samples = get_sample_predictions(\n",
        "    trainer.model,\n",
        "    eval_dataset,\n",
        "    vocab,\n",
        "    device=device,\n",
        "    num_samples=10\n",
        ")\n",
        "\n",
        "print_sample_predictions(samples, num_to_print=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nModel: Transformer (Memory-Efficient)\")\n",
        "print(f\"  - d_model: 256\")\n",
        "print(f\"  - num_heads: 4\")\n",
        "print(f\"  - encoder_layers: 2\")\n",
        "print(f\"  - decoder_layers: 2\")\n",
        "print(f\"  - Parameters: {total_params:,}\")\n",
        "\n",
        "print(f\"\\nBest validation loss: {min(history['val_loss']):.4f}\")\n",
        "\n",
        "print(f\"\\nEvaluation metrics:\")\n",
        "print(f\"  - Repetition rate: {results['avg_repetition_rate']:.4f}\")\n",
        "print(f\"  - Vocabulary diversity: {results['vocabulary_diversity']:.4f}\")\n",
        "print(f\"  - Mean caption length: {results['mean_caption_length']:.2f} words\")\n",
        "\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  ✓ ../checkpoints/best_transformer_small.pth\")\n",
        "print(f\"  ✓ ../results/transformer_small_history.json\")\n",
        "print(f\"  ✓ ../results/transformer_small_results.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Next: Compare all models with 05_evaluate_all.ipynb\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
