{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference-Based Evaluation of Audio Captioning Models\n",
    "\n",
    "This notebook performs comprehensive evaluation using **reference-based metrics**:\n",
    "\n",
    "- **BLEU (1-4)**: N-gram overlap with references\n",
    "- **METEOR**: Alignment-based metric with synonyms consideration\n",
    "- **ROUGE-L**: Longest common subsequence\n",
    "- **CIDEr**: Consensus-based metric using TF-IDF\n",
    "\n",
    "These metrics compare generated captions against human-written references, providing quantitative measures of caption quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['MKL_THREADING_LAYER'] = 'GNU'\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nimport sys\nfrom pathlib import Path\n\n# Add parent directory to path\nproject_root = Path('..').absolute()\nsys.path.insert(0, str(project_root))\n\nimport torch\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()\n\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom src.models import create_model\nfrom src.dataset import create_dataloaders\nfrom src.utils import load_vocab, set_seed, get_device\nfrom src.model_configs import load_model_from_checkpoint\nfrom src.reference_metrics import (\n    compute_all_metrics,\n    evaluate_captions,\n    print_metrics,\n    bleu_score,\n    meteor_score,\n    rouge_l_score,\n    cider_score\n)\n\nprint(\"✓ Imports successful!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test Reference Metrics on Examples\n",
    "\n",
    "Let's first test the metrics on some example captions to understand what they measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing reference-based metrics on example captions...\\n\")\n",
    "\n",
    "# Example references (ground truth)\n",
    "references = [\n",
    "    \"a dog is barking loudly in the backyard\",\n",
    "    \"the dog barks outside in the yard\",\n",
    "    \"loud barking from a dog in an outdoor area\"\n",
    "]\n",
    "\n",
    "print(\"Reference captions:\")\n",
    "for i, ref in enumerate(references, 1):\n",
    "    print(f\"  {i}. {ref}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Test different candidate qualities\n",
    "test_cases = [\n",
    "    (\"Perfect match\", \"a dog is barking loudly in the backyard\"),\n",
    "    (\"Good paraphrase\", \"a dog barking in the yard\"),\n",
    "    (\"Partial match\", \"a dog in the backyard\"),\n",
    "    (\"Related but different\", \"a cat is meowing in the house\"),\n",
    "    (\"Completely wrong\", \"cars driving on a highway\")\n",
    "]\n",
    "\n",
    "for label, candidate in test_cases:\n",
    "    print(f\"\\n{label}: '{candidate}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metrics = compute_all_metrics(candidate, references)\n",
    "    \n",
    "    # Print in one line\n",
    "    print(f\"  BLEU-1: {metrics['BLEU-1']:.3f}  |  \"\n",
    "          f\"BLEU-4: {metrics['BLEU-4']:.3f}  |  \"\n",
    "          f\"METEOR: {metrics['METEOR']:.3f}  |  \"\n",
    "          f\"ROUGE-L: {metrics['ROUGE-L']:.3f}  |  \"\n",
    "          f\"CIDEr: {metrics['CIDEr']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocab = load_vocab('../vocab.json')\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Create reverse mapping\n",
    "idx_to_word = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load eval dataset\n",
    "_, _, eval_dataset = create_dataloaders(\n",
    "    train_captions='../data/train_captions.json',\n",
    "    val_captions='../data/val_captions.json',\n",
    "    eval_captions='../data/eval_captions.json',\n",
    "    train_features_dir='../features/mel/',\n",
    "    val_features_dir='../features/mel/',\n",
    "    eval_features_dir='../features/mel_eval/',\n",
    "    vocab=vocab,\n",
    "    batch_size=8,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Trained Models\n",
    "\n",
    "We'll evaluate all available trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os.path as osp\n\n# Check for available checkpoints\ncheckpoint_dir = '../checkpoints'\navailable_checkpoints = []\n\nif osp.exists(checkpoint_dir):\n    for fname in os.listdir(checkpoint_dir):\n        if fname.endswith('.pth'):\n            available_checkpoints.append(osp.join(checkpoint_dir, fname))\n\nprint(f\"Found {len(available_checkpoints)} checkpoint(s):\")\nfor cp in available_checkpoints:\n    print(f\"  - {osp.basename(cp)}\")\n\n# Load all available models using automatic config detection\nmodels = {}\n\nfor checkpoint_path in available_checkpoints:\n    checkpoint_name = osp.basename(checkpoint_path)\n    \n    # Determine model name from checkpoint filename\n    if 'baseline_small' in checkpoint_name:\n        model_name = 'baseline_small'\n    elif 'baseline' in checkpoint_name and 'improved' not in checkpoint_name:\n        model_name = 'baseline'\n    elif 'improved_baseline' in checkpoint_name:\n        model_name = 'improved_baseline'\n    elif 'attention_small' in checkpoint_name:\n        model_name = 'attention_small'\n    elif 'attention' in checkpoint_name:\n        model_name = 'attention'\n    elif 'transformer_small' in checkpoint_name:\n        model_name = 'transformer_small'\n    elif 'transformer' in checkpoint_name:\n        model_name = 'transformer'\n    else:\n        # Try to use filename without extension\n        model_name = checkpoint_name.replace('best_', '').replace('.pth', '')\n    \n    print(f\"\\nLoading {model_name} from {checkpoint_name}...\")\n    \n    try:\n        # Use automatic config detection\n        model, config, checkpoint = load_model_from_checkpoint(\n            checkpoint_path,\n            vocab_size=len(vocab),\n            device=device\n        )\n        \n        model.eval()\n        models[model_name] = model\n        \n        print(f\"  ✓ Successfully loaded {model_name}\")\n        print(f\"  Config: {config}\")\n        \n    except Exception as e:\n        print(f\"  ✗ Failed to load {model_name}: {e}\")\n        continue\n\nprint(f\"\\n{'='*80}\")\nprint(f\"Successfully loaded {len(models)} model(s): {list(models.keys())}\")\nprint(f\"{'='*80}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Captions for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids, vocab):\n",
    "    \"\"\"Convert token IDs to text\"\"\"\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    words = []\n",
    "    for idx in token_ids:\n",
    "        idx = idx.item() if torch.is_tensor(idx) else idx\n",
    "        if idx == vocab['<eos>']:\n",
    "            break\n",
    "        if idx not in [vocab['<pad>'], vocab['<sos>'], 0]:\n",
    "            words.append(idx_to_word.get(idx, '<unk>'))\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def generate_captions_for_model(model, eval_dataset, vocab, device, num_samples=100, \n",
    "                                 temperature=0.7, use_sampling=True):\n",
    "    \"\"\"\n",
    "    Generate captions for evaluation dataset\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        eval_dataset: Evaluation dataset\n",
    "        vocab: Vocabulary\n",
    "        device: Device\n",
    "        num_samples: Number of samples to evaluate\n",
    "        temperature: Sampling temperature (lower = more conservative)\n",
    "        use_sampling: If False, use greedy decoding\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    generated_captions = []\n",
    "    reference_captions = []\n",
    "    \n",
    "    num_samples = min(num_samples, len(eval_dataset))\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=\"Generating\"):\n",
    "        item = eval_dataset[i]\n",
    "        mel = item['mel'].unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if use_sampling:\n",
    "                ids = model.generate(\n",
    "                    mel,\n",
    "                    max_len=30,\n",
    "                    sos_idx=vocab['<sos>'],\n",
    "                    eos_idx=vocab['<eos>'],\n",
    "                    temperature=temperature,\n",
    "                    top_k=50,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "            else:\n",
    "                # Greedy (very low temperature)\n",
    "                ids = model.generate(\n",
    "                    mel,\n",
    "                    max_len=30,\n",
    "                    sos_idx=vocab['<sos>'],\n",
    "                    eos_idx=vocab['<eos>'],\n",
    "                    temperature=0.1,\n",
    "                    top_p=1.0\n",
    "                )\n",
    "        \n",
    "        caption = decode_tokens(ids[0], vocab)\n",
    "        generated_captions.append(caption)\n",
    "        reference_captions.append(item['captions'])\n",
    "    \n",
    "    return generated_captions, reference_captions\n",
    "\n",
    "\n",
    "# Generate captions for all models\n",
    "print(\"Generating captions for all models...\\n\")\n",
    "\n",
    "all_generated = {}\n",
    "references = None  # Same for all models\n",
    "\n",
    "NUM_EVAL_SAMPLES = 200  # Adjust based on your needs\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nGenerating for {model_name}...\")\n",
    "    generated, refs = generate_captions_for_model(\n",
    "        model, eval_dataset, vocab, device,\n",
    "        num_samples=NUM_EVAL_SAMPLES,\n",
    "        temperature=0.7,\n",
    "        use_sampling=True\n",
    "    )\n",
    "    all_generated[model_name] = generated\n",
    "    \n",
    "    if references is None:\n",
    "        references = refs\n",
    "    \n",
    "    print(f\"  Generated {len(generated)} captions\")\n",
    "\n",
    "print(\"\\n✓ Caption generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate with Reference-Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating all models with reference-based metrics...\\n\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_name, generated in all_generated.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    metrics = evaluate_captions(generated, references)\n",
    "    all_results[model_name] = metrics\n",
    "    \n",
    "    print_metrics(metrics, f\"{model_name.upper()} Results\")\n",
    "\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comparison Table and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "df = pd.DataFrame(all_results).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON - REFERENCE-BASED METRICS\")\n",
    "print(\"=\"*100)\n",
    "print(df.to_string())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('../results/reference_based_metrics_comparison.csv')\n",
    "print(\"\\n✓ Saved to results/reference_based_metrics_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Reference-Based Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'METEOR', 'ROUGE-L']\n",
    "colors = plt.cm.Set3(range(len(models)))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    values = [all_results[model][metric] for model in models.keys()]\n",
    "    bars = ax.bar(range(len(models)), values, color=colors)\n",
    "    \n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models.keys(), rotation=45, ha='right')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(metric, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/reference_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved visualization to results/reference_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIDEr comparison (separate plot as it has different scale)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "cider_values = [all_results[model]['CIDEr'] for model in models.keys()]\n",
    "bars = ax.bar(range(len(models)), cider_values, color=colors)\n",
    "\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models.keys(), rotation=45, ha='right')\n",
    "ax.set_ylabel('CIDEr Score', fontsize=12)\n",
    "ax.set_title('CIDEr Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/cider_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved CIDEr visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Sample Predictions with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SAMPLE PREDICTIONS WITH REFERENCE-BASED SCORES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "num_samples_to_show = 5\n",
    "\n",
    "for i in range(num_samples_to_show):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"SAMPLE {i+1}: {eval_dataset[i]['fname']}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Show references\n",
    "    print(\"\\nReferences:\")\n",
    "    for j, ref in enumerate(references[i], 1):\n",
    "        print(f\"  {j}. {ref}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    \n",
    "    # Show each model's prediction and scores\n",
    "    for model_name in models.keys():\n",
    "        candidate = all_generated[model_name][i]\n",
    "        metrics = compute_all_metrics(candidate, references[i])\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()}:\")\n",
    "        print(f\"  Generated: {candidate}\")\n",
    "        print(f\"  Scores: BLEU-4={metrics['BLEU-4']:.3f} | \"\n",
    "              f\"METEOR={metrics['METEOR']:.3f} | \"\n",
    "              f\"ROUGE-L={metrics['ROUGE-L']:.3f} | \"\n",
    "              f\"CIDEr={metrics['CIDEr']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-sample metrics for statistical analysis\n",
    "print(\"Computing per-sample metrics for statistical analysis...\\n\")\n",
    "\n",
    "from src.reference_metrics import _compute_doc_freq\n",
    "\n",
    "# Compute doc freq once\n",
    "doc_freq = _compute_doc_freq(references)\n",
    "num_docs = len(references)\n",
    "\n",
    "per_sample_metrics = {}\n",
    "\n",
    "for model_name, generated in all_generated.items():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "    \n",
    "    sample_metrics = []\n",
    "    for candidate, refs in zip(generated, references):\n",
    "        metrics = compute_all_metrics(candidate, refs, doc_freq, num_docs)\n",
    "        sample_metrics.append(metrics)\n",
    "    \n",
    "    per_sample_metrics[model_name] = sample_metrics\n",
    "\n",
    "print(\"\\n✓ Per-sample metrics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STATISTICAL SUMMARY (Mean ± Std)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for metric in ['BLEU-4', 'METEOR', 'ROUGE-L', 'CIDEr']:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        scores = [m[metric] for m in per_sample_metrics[model_name]]\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        print(f\"  {model_name:.<30} {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for distribution visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Score Distributions Across Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_for_box = ['BLEU-4', 'METEOR', 'ROUGE-L', 'CIDEr']\n",
    "\n",
    "for idx, metric in enumerate(metrics_for_box):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        scores = [m[metric] for m in per_sample_metrics[model_name]]\n",
    "        data.append(scores)\n",
    "        labels.append(model_name)\n",
    "    \n",
    "    bp = ax.boxplot(data, labels=labels, patch_artist=True)\n",
    "    \n",
    "    # Color boxes\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax.set_ylabel('Score', fontsize=11)\n",
    "    ax.set_title(metric, fontweight='bold', fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved distribution plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "results_to_save = {\n",
    "    'num_samples': NUM_EVAL_SAMPLES,\n",
    "    'models_evaluated': list(models.keys()),\n",
    "    'averaged_metrics': all_results,\n",
    "    'per_sample_metrics': per_sample_metrics,\n",
    "    'generated_captions': all_generated,\n",
    "    'reference_captions': references\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open('../results/reference_based_evaluation_complete.json', 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved complete results to results/reference_based_evaluation_complete.json\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'num_samples': NUM_EVAL_SAMPLES,\n",
    "    'models': list(models.keys()),\n",
    "    'metrics': all_results\n",
    "}\n",
    "\n",
    "with open('../results/reference_metrics_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved summary to results/reference_metrics_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Metrics Explained:\n",
    "\n",
    "1. **BLEU (1-4)**: Measures n-gram precision (1-4 words)\n",
    "   - Higher = more word overlap with references\n",
    "   - BLEU-4 most commonly used\n",
    "   - Range: 0-1 (higher is better)\n",
    "\n",
    "2. **METEOR**: Alignment-based with stemming/synonyms\n",
    "   - Balances precision and recall\n",
    "   - More flexible than BLEU\n",
    "   - Range: 0-1 (higher is better)\n",
    "\n",
    "3. **ROUGE-L**: Longest common subsequence\n",
    "   - Captures sentence-level structure\n",
    "   - Order-aware unlike BLEU\n",
    "   - Range: 0-1 (higher is better)\n",
    "\n",
    "4. **CIDEr**: Consensus-based using TF-IDF\n",
    "   - Weights rare/informative words higher\n",
    "   - Standard for image/audio captioning\n",
    "   - Range: 0-10 typically (higher is better)\n",
    "\n",
    "### What Good Scores Look Like:\n",
    "\n",
    "- **BLEU-4**: >0.15 (good), >0.25 (very good)\n",
    "- **METEOR**: >0.20 (good), >0.30 (very good)\n",
    "- **ROUGE-L**: >0.35 (good), >0.45 (very good)\n",
    "- **CIDEr**: >0.5 (good), >1.0 (very good)\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- `results/reference_based_metrics_comparison.csv` - Comparison table\n",
    "- `results/reference_metrics_comparison.png` - Metrics visualization\n",
    "- `results/cider_comparison.png` - CIDEr comparison\n",
    "- `results/score_distributions.png` - Box plots\n",
    "- `results/reference_based_evaluation_complete.json` - Full results\n",
    "- `results/reference_metrics_summary.json` - Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}