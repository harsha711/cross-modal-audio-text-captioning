{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model with Beam Search\n",
    "\n",
    "This notebook implements and evaluates the transformer model with **beam search decoding** for improved caption generation.\n",
    "\n",
    "## Beam Search vs. Greedy/Sampling\n",
    "\n",
    "**Greedy decoding**: Picks the most likely token at each step (fast, but locally optimal)\n",
    "\n",
    "**Sampling**: Uses temperature/nucleus sampling for diversity (creative, but less coherent)\n",
    "\n",
    "**Beam search**: Maintains top-k hypotheses, exploring multiple paths (better quality, more coherent)\n",
    "\n",
    "## What's Covered:\n",
    "1. Implementing beam search for transformer model\n",
    "2. Training the transformer with memory-efficient settings\n",
    "3. Comparing beam search vs greedy vs sampling\n",
    "4. Analyzing results and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "project_root = Path('..').absolute()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from src.models import create_model, TransformerModel\n",
    "from src.dataset import create_dataloaders\n",
    "from src.trainer import ModelTrainer\n",
    "from src.utils import load_vocab, set_seed, get_device, count_parameters, make_json_serializable\n",
    "\n",
    "print(\"✓ Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Beam Search\n",
    "\n",
    "Beam search maintains the top-k most likely sequences at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_generate(model, mel, beam_width=5, max_len=30, sos_idx=1, eos_idx=2, \n",
    "                         length_penalty=0.6, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate captions using beam search\n",
    "    \n",
    "    Args:\n",
    "        model: TransformerModel instance\n",
    "        mel: Audio mel spectrogram (batch, 1, 64, 3000)\n",
    "        beam_width: Number of beams to maintain\n",
    "        max_len: Maximum caption length\n",
    "        sos_idx: Start-of-sequence token index\n",
    "        eos_idx: End-of-sequence token index\n",
    "        length_penalty: Length normalization penalty (0.6-1.0 typical)\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        best_sequences: (batch, max_len) generated token IDs\n",
    "        best_scores: (batch,) log probabilities of best sequences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = mel.size(0)\n",
    "    \n",
    "    # Encode audio once (shared across all beams)\n",
    "    with torch.no_grad():\n",
    "        audio_features = model.encode_audio(mel)  # (batch, audio_len, d_model)\n",
    "    \n",
    "    # Initialize beams for each item in batch\n",
    "    # Each beam: (sequence, score)\n",
    "    all_best_sequences = []\n",
    "    all_best_scores = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        # Get audio features for this batch item\n",
    "        audio_feat = audio_features[b:b+1]  # (1, audio_len, d_model)\n",
    "        \n",
    "        # Initialize beams: each beam is (sequence, score)\n",
    "        beams = [(torch.tensor([sos_idx], device=device), 0.0)]\n",
    "        completed_beams = []\n",
    "        \n",
    "        for step in range(max_len):\n",
    "            candidates = []\n",
    "            \n",
    "            for seq, score in beams:\n",
    "                # Skip if sequence already ended\n",
    "                if seq[-1].item() == eos_idx:\n",
    "                    completed_beams.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                # Prepare input: (1, current_len)\n",
    "                seq_input = seq.unsqueeze(0)\n",
    "                \n",
    "                # Generate next token probabilities\n",
    "                with torch.no_grad():\n",
    "                    # Embed and add positional encoding\n",
    "                    embedded = model.embedding(seq_input) * math.sqrt(model.d_model)\n",
    "                    embedded = model.pos_encoder(embedded)\n",
    "                    \n",
    "                    # Create causal mask\n",
    "                    tgt_mask = model.generate_square_subsequent_mask(seq_input.size(1), device)\n",
    "                    \n",
    "                    # Decode\n",
    "                    output = model.transformer(\n",
    "                        src=audio_feat,\n",
    "                        tgt=embedded,\n",
    "                        tgt_mask=tgt_mask\n",
    "                    )\n",
    "                    \n",
    "                    # Get logits for last token\n",
    "                    logits = model.output_projection(output[0, -1, :])  # (vocab_size,)\n",
    "                    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top-k next tokens\n",
    "                top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "                \n",
    "                # Create new candidate beams\n",
    "                for log_prob, idx in zip(top_log_probs, top_indices):\n",
    "                    new_seq = torch.cat([seq, idx.unsqueeze(0)])\n",
    "                    new_score = score + log_prob.item()\n",
    "                    candidates.append((new_seq, new_score))\n",
    "            \n",
    "            # If no candidates, break\n",
    "            if not candidates:\n",
    "                break\n",
    "            \n",
    "            # Select top beam_width candidates\n",
    "            # Apply length penalty to scores\n",
    "            candidates_with_penalty = []\n",
    "            for seq, score in candidates:\n",
    "                length_norm = ((5 + len(seq)) / 6) ** length_penalty\n",
    "                normalized_score = score / length_norm\n",
    "                candidates_with_penalty.append((seq, score, normalized_score))\n",
    "            \n",
    "            # Sort by normalized score and keep top beam_width\n",
    "            candidates_with_penalty.sort(key=lambda x: x[2], reverse=True)\n",
    "            beams = [(seq, score) for seq, score, _ in candidates_with_penalty[:beam_width]]\n",
    "            \n",
    "            # Check if all beams ended\n",
    "            if all(seq[-1].item() == eos_idx for seq, _ in beams):\n",
    "                completed_beams.extend(beams)\n",
    "                break\n",
    "        \n",
    "        # Add remaining beams to completed\n",
    "        completed_beams.extend(beams)\n",
    "        \n",
    "        # Select best beam with length normalization\n",
    "        best_beam = None\n",
    "        best_normalized_score = float('-inf')\n",
    "        \n",
    "        for seq, score in completed_beams:\n",
    "            length_norm = ((5 + len(seq)) / 6) ** length_penalty\n",
    "            normalized_score = score / length_norm\n",
    "            if normalized_score > best_normalized_score:\n",
    "                best_normalized_score = normalized_score\n",
    "                best_beam = (seq, score)\n",
    "        \n",
    "        # Store best sequence\n",
    "        if best_beam:\n",
    "            all_best_sequences.append(best_beam[0])\n",
    "            all_best_scores.append(best_beam[1])\n",
    "        else:\n",
    "            all_best_sequences.append(torch.tensor([sos_idx, eos_idx], device=device))\n",
    "            all_best_scores.append(0.0)\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    max_seq_len = max(len(seq) for seq in all_best_sequences)\n",
    "    padded_sequences = []\n",
    "    \n",
    "    for seq in all_best_sequences:\n",
    "        if len(seq) < max_seq_len:\n",
    "            padding = torch.zeros(max_seq_len - len(seq), dtype=torch.long, device=device)\n",
    "            seq = torch.cat([seq, padding])\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    best_sequences = torch.stack(padded_sequences)\n",
    "    best_scores = torch.tensor(all_best_scores, device=device)\n",
    "    \n",
    "    # Remove <sos> token from output\n",
    "    best_sequences = best_sequences[:, 1:]\n",
    "    \n",
    "    return best_sequences, best_scores\n",
    "\n",
    "\n",
    "# Add beam search method to TransformerModel\n",
    "TransformerModel.beam_search_generate = beam_search_generate\n",
    "\n",
    "print(\"✓ Beam search implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "device = get_device()\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    free_memory = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Free GPU memory: {free_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No GPU available, using CPU (will be slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading vocabulary...\")\n",
    "vocab = load_vocab('../vocab.json')\n",
    "print(f\"\\n✓ Vocabulary size: {len(vocab)}\")\n",
    "print(f\"  <pad>: {vocab['<pad>']}\")\n",
    "print(f\"  <sos>: {vocab['<sos>']}\")\n",
    "print(f\"  <eos>: {vocab['<eos>']}\")\n",
    "print(f\"  <unk>: {vocab['<unk>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dataloaders...\")\n",
    "print(\"Using batch_size=8 for transformer\\n\")\n",
    "\n",
    "train_loader, val_loader, eval_dataset = create_dataloaders(\n",
    "    train_captions='../data/train_captions.json',\n",
    "    val_captions='../data/val_captions.json',\n",
    "    eval_captions='../data/eval_captions.json',\n",
    "    train_features_dir='../features/mel/',\n",
    "    val_features_dir='../features/mel/',\n",
    "    eval_features_dir='../features/mel_eval/',\n",
    "    vocab=vocab,\n",
    "    batch_size=8,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataloaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Eval samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create or Load Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "# Check if pre-trained model exists\n",
    "pretrained_path = '../checkpoints/best_transformer_small.pth'\n",
    "\n",
    "if osp.exists(pretrained_path):\n",
    "    print(f\"Loading pre-trained model from {pretrained_path}...\\n\")\n",
    "    \n",
    "    model = create_model(\n",
    "        'transformer',\n",
    "        vocab_size=len(vocab),\n",
    "        d_model=256,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dim_feedforward=512\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    checkpoint = torch.load(pretrained_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"✓ Loaded pre-trained model\")\n",
    "    \n",
    "else:\n",
    "    print(\"No pre-trained model found. Creating and training new model...\\n\")\n",
    "    print(\"Model configuration (optimized for 8GB GPU):\")\n",
    "    print(\"  - d_model: 256\")\n",
    "    print(\"  - num_heads: 4\")\n",
    "    print(\"  - num_encoder_layers: 2\")\n",
    "    print(\"  - num_decoder_layers: 2\")\n",
    "    print(\"  - dim_feedforward: 512\\n\")\n",
    "    \n",
    "    model = create_model(\n",
    "        'transformer',\n",
    "        vocab_size=len(vocab),\n",
    "        d_model=256,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dim_feedforward=512\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nModel parameters: {total_params:,} total, {trainable_params:,} trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: (Optional) Train Model if Not Pre-trained\n",
    "\n",
    "Skip this if you loaded a pre-trained model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if model is not pre-trained\n",
    "if not osp.exists(pretrained_path):\n",
    "    print(\"Training transformer model...\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"TRANSFORMER MODEL TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nExpected training time: 4-6 hours on GPU\\n\")\n",
    "    \n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        vocab=vocab,\n",
    "        device=device,\n",
    "        model_name='transformer_beam'\n",
    "    )\n",
    "    \n",
    "    history = trainer.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        eval_dataset=eval_dataset,\n",
    "        num_epochs=40,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        patience=5,\n",
    "        label_smoothing=0.1,\n",
    "        save_dir='../checkpoints'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    \n",
    "    # Save history\n",
    "    with open('../results/transformer_beam_history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "else:\n",
    "    print(\"Using pre-trained model. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Beam Search on Sample\n",
    "\n",
    "Let's test beam search on a single sample and compare with greedy/sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids, vocab):\n",
    "    \"\"\"Convert token IDs to text\"\"\"\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    words = []\n",
    "    for idx in token_ids:\n",
    "        idx = idx.item() if torch.is_tensor(idx) else idx\n",
    "        if idx == vocab['<eos>']:\n",
    "            break\n",
    "        if idx not in [vocab['<pad>'], vocab['<sos>'], 0]:\n",
    "            words.append(idx_to_word.get(idx, '<unk>'))\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# Get a sample\n",
    "sample_idx = 0\n",
    "sample = eval_dataset[sample_idx]\n",
    "mel = sample['mel'].unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"Sample: {sample['fname']}\")\n",
    "print(f\"\\nReference captions:\")\n",
    "for i, ref in enumerate(sample['captions'], 1):\n",
    "    print(f\"  {i}. {ref}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATION METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 1. Greedy decoding (temperature=0 approximation, or argmax)\n",
    "print(\"\\n1. GREEDY DECODING\")\n",
    "with torch.no_grad():\n",
    "    greedy_ids = model.generate(\n",
    "        mel, max_len=30, \n",
    "        sos_idx=vocab['<sos>'], \n",
    "        eos_idx=vocab['<eos>'],\n",
    "        temperature=0.1,  # Very low temperature ≈ greedy\n",
    "        top_p=1.0\n",
    "    )\n",
    "greedy_caption = decode_tokens(greedy_ids[0], vocab)\n",
    "print(f\"   {greedy_caption}\")\n",
    "\n",
    "# 2. Sampling with temperature\n",
    "print(\"\\n2. SAMPLING (temperature=0.7, top_p=0.9)\")\n",
    "with torch.no_grad():\n",
    "    sample_ids = model.generate(\n",
    "        mel, max_len=30,\n",
    "        sos_idx=vocab['<sos>'],\n",
    "        eos_idx=vocab['<eos>'],\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9\n",
    "    )\n",
    "sample_caption = decode_tokens(sample_ids[0], vocab)\n",
    "print(f\"   {sample_caption}\")\n",
    "\n",
    "# 3. Beam search\n",
    "print(\"\\n3. BEAM SEARCH (beam_width=5)\")\n",
    "with torch.no_grad():\n",
    "    beam_ids, beam_scores = beam_search_generate(\n",
    "        model, mel,\n",
    "        beam_width=5,\n",
    "        max_len=30,\n",
    "        sos_idx=vocab['<sos>'],\n",
    "        eos_idx=vocab['<eos>'],\n",
    "        length_penalty=0.6,\n",
    "        device=device\n",
    "    )\n",
    "beam_caption = decode_tokens(beam_ids[0], vocab)\n",
    "print(f\"   {beam_caption}\")\n",
    "print(f\"   Score: {beam_scores[0].item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comprehensive Evaluation with Beam Search\n",
    "\n",
    "Evaluate on multiple samples using beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_beam_search(model, eval_dataset, vocab, device='cuda', \n",
    "                               num_samples=100, beam_width=5, length_penalty=0.6):\n",
    "    \"\"\"\n",
    "    Evaluate model using beam search\n",
    "    \"\"\"\n",
    "    from src.evaluation import calculate_repetition_rate, evaluate_diversity, calculate_caption_length_stats\n",
    "    \n",
    "    model.eval()\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    generated_captions = []\n",
    "    reference_captions = []\n",
    "    repetition_scores = []\n",
    "    \n",
    "    num_samples = min(num_samples, len(eval_dataset))\n",
    "    \n",
    "    print(f\"Evaluating with beam search (beam_width={beam_width})...\")\n",
    "    \n",
    "    for i in tqdm(range(num_samples)):\n",
    "        item = eval_dataset[i]\n",
    "        mel = item['mel'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate with beam search\n",
    "        with torch.no_grad():\n",
    "            ids, scores = beam_search_generate(\n",
    "                model, mel,\n",
    "                beam_width=beam_width,\n",
    "                max_len=30,\n",
    "                sos_idx=vocab['<sos>'],\n",
    "                eos_idx=vocab['<eos>'],\n",
    "                length_penalty=length_penalty,\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        caption = decode_tokens(ids[0], vocab)\n",
    "        generated_captions.append(caption)\n",
    "        reference_captions.append(item['captions'])\n",
    "        repetition_scores.append(calculate_repetition_rate(caption))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    diversity_metrics = evaluate_diversity(generated_captions)\n",
    "    length_stats = calculate_caption_length_stats(generated_captions)\n",
    "    avg_repetition = np.mean(repetition_scores)\n",
    "    \n",
    "    results = {\n",
    "        'num_samples': num_samples,\n",
    "        'beam_width': beam_width,\n",
    "        'length_penalty': length_penalty,\n",
    "        'avg_repetition_rate': avg_repetition,\n",
    "        'vocabulary_diversity': diversity_metrics['diversity'],\n",
    "        'unique_words_used': diversity_metrics['unique_words'],\n",
    "        'total_words_generated': diversity_metrics['total_words'],\n",
    "        'mean_caption_length': length_stats['mean_length'],\n",
    "        'std_caption_length': length_stats['std_length'],\n",
    "        'min_caption_length': length_stats['min_length'],\n",
    "        'max_caption_length': length_stats['max_length']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BEAM SEARCH EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key:.<40} {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key:.<40} {value}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results, generated_captions, reference_captions\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "beam_results, beam_captions, beam_refs = evaluate_with_beam_search(\n",
    "    model, eval_dataset, vocab, \n",
    "    device=device,\n",
    "    num_samples=100,\n",
    "    beam_width=5,\n",
    "    length_penalty=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Different Beam Widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing different beam widths...\\n\")\n",
    "\n",
    "beam_widths = [1, 3, 5, 10]\n",
    "comparison_results = {}\n",
    "\n",
    "for beam_width in beam_widths:\n",
    "    print(f\"\\nTesting beam_width={beam_width}...\")\n",
    "    results, captions, refs = evaluate_with_beam_search(\n",
    "        model, eval_dataset, vocab,\n",
    "        device=device,\n",
    "        num_samples=50,  # Use fewer samples for faster comparison\n",
    "        beam_width=beam_width,\n",
    "        length_penalty=0.6\n",
    "    )\n",
    "    comparison_results[f\"beam_{beam_width}\"] = results\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BEAM WIDTH COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Beam Width':<15} {'Repetition':<15} {'Diversity':<15} {'Avg Length':<15} {'Vocab Used':<15}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for name, results in comparison_results.items():\n",
    "    beam_w = results['beam_width']\n",
    "    print(f\"{beam_w:<15} {results['avg_repetition_rate']:<15.4f} \"\n",
    "          f\"{results['vocabulary_diversity']:<15.4f} {results['mean_caption_length']:<15.2f} \"\n",
    "          f\"{results['unique_words_used']:<15}\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Sample Predictions with Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS WITH BEAM SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_samples_to_show = 10\n",
    "\n",
    "for i in range(min(num_samples_to_show, len(eval_dataset))):\n",
    "    item = eval_dataset[i]\n",
    "    mel = item['mel'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate with beam search\n",
    "    with torch.no_grad():\n",
    "        beam_ids, beam_scores = beam_search_generate(\n",
    "            model, mel,\n",
    "            beam_width=5,\n",
    "            max_len=30,\n",
    "            sos_idx=vocab['<sos>'],\n",
    "            eos_idx=vocab['<eos>'],\n",
    "            length_penalty=0.6,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    caption = decode_tokens(beam_ids[0], vocab)\n",
    "    \n",
    "    print(f\"\\nSample {i+1}: {item['fname']}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Generated:  {caption}\")\n",
    "    print(f\"Score:      {beam_scores[0].item():.4f}\")\n",
    "    print(f\"References:\")\n",
    "    for j, ref in enumerate(item['captions'], 1):\n",
    "        print(f\"  {j}. {ref}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save beam search results\n",
    "serializable_results = make_json_serializable(beam_results)\n",
    "\n",
    "with open('../results/transformer_beam_search_results.json', 'w') as f:\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "# Save beam width comparison\n",
    "serializable_comparison = {k: make_json_serializable(v) for k, v in comparison_results.items()}\n",
    "\n",
    "with open('../results/beam_width_comparison.json', 'w') as f:\n",
    "    json.dump(serializable_comparison, f, indent=2)\n",
    "\n",
    "print(\"✓ Results saved!\")\n",
    "print(\"  - ../results/transformer_beam_search_results.json\")\n",
    "print(\"  - ../results/beam_width_comparison.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Beam search implementation** for transformer models\n",
    "2. **Comparison** between greedy, sampling, and beam search decoding\n",
    "3. **Effect of beam width** on generation quality\n",
    "4. **Comprehensive evaluation** with beam search\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- **Beam search** generally produces more coherent and natural captions than greedy decoding\n",
    "- **Larger beam widths** (5-10) typically give better results but are slower\n",
    "- **Length penalty** (0.6) helps avoid overly short captions\n",
    "- Beam search often has **lower repetition** and **better vocabulary diversity** than sampling\n",
    "\n",
    "### Trade-offs:\n",
    "\n",
    "- **Greedy**: Fastest, but locally optimal\n",
    "- **Sampling**: Creative and diverse, but less coherent\n",
    "- **Beam search**: Best quality, but slower (especially with large beams)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different length penalties (0.4, 0.6, 0.8, 1.0)\n",
    "- Experiment with diverse beam search (group beam search)\n",
    "- Combine with temperature sampling for controllable diversity\n",
    "- Compare with reference-based metrics (BLEU, METEOR, CIDEr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
