{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911117c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a91a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import os, numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee424f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aac_datasets import Clotho\n",
    "\n",
    "# Add download=True for each to download the datasets if not already present\n",
    "train_dataset = Clotho(root=\".\", subset=\"dev\")\n",
    "eval_dataset = Clotho(root=\".\", subset=\"eval\")\n",
    "val_dataset = Clotho(root=\".\", subset=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086f0bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A muddled noise of broken channel of the TV',\n",
       " 'A television blares the rhythm of a static TV.',\n",
       " 'Loud television static dips in and out of focus',\n",
       " 'The loud buzz of static constantly changes pitch and volume.',\n",
       " 'heavy static and the beginnings of a signal on a transistor radio']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "audio, captions = item[\"audio\"], item[\"captions\"]\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e80f7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3839, 1045, 1045)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(eval_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d524966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions: ['A muddled noise of broken channel of the TV', 'A television blares the rhythm of a static TV.', 'Loud television static dips in and out of focus', 'The loud buzz of static constantly changes pitch and volume.', 'heavy static and the beginnings of a signal on a transistor radio']\n",
      "Audio shape: torch.Size([1, 1153825])\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(\"Captions:\", sample[\"captions\"])\n",
    "print(\"Audio shape:\", sample[\"audio\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57325c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=44100,   # Clotho uses 44.1kHz audio\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "to_db = torchaudio.transforms.AmplitudeToDB(stype='power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe399c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mel_spectrograms(dataset, split_name):\n",
    "    if os.path.exists(f\"features/mel_{split_name}\") and len(os.listdir(f\"features/mel_{split_name}\")) == len(dataset):\n",
    "        print(f\"✅ Spectrograms already exist. Skipping generation.\")\n",
    "    else:\n",
    "        print(\"Generating spectrograms...\")\n",
    "        os.makedirs(f\"features/mel_{split_name}\", exist_ok=True)\n",
    "        \n",
    "        for item in tqdm(dataset):\n",
    "            waveform = item[\"audio\"]\n",
    "            \n",
    "            # Normalize to [1, samples] regardless of input\n",
    "            if waveform.dim() == 1:  # [samples] → [1, samples]\n",
    "                waveform = waveform.unsqueeze(0)\n",
    "            elif waveform.dim() == 2 and waveform.shape[0] == 2:  # [2, samples] → [1, samples]\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)  # Convert stereo to mono\n",
    "            elif waveform.dim() == 2 and waveform.shape[0] != 1:\n",
    "                waveform = waveform[0:1]  # Take first channel\n",
    "            \n",
    "            # Now waveform is guaranteed to be [1, samples]\n",
    "            mel = mel_transform(waveform)  # → [1, 64, time]\n",
    "            mel_db = to_db(mel)\n",
    "            \n",
    "            # Save with consistent shape [1, 64, time]\n",
    "            assert mel_db.dim() == 3 and mel_db.shape[0] == 1, f\"Unexpected shape: {mel_db.shape}\"\n",
    "            np.save(f\"features/mel_{split_name}/{item['fname']}.npy\", mel_db.numpy())\n",
    "        \n",
    "        print(\"✅ Spectrograms generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fc6d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spectrograms already exist. Skipping generation.\n",
      "✅ Spectrograms already exist. Skipping generation.\n",
      "✅ Spectrograms already exist. Skipping generation.\n"
     ]
    }
   ],
   "source": [
    "generate_mel_spectrograms(train_dataset, \"train\")\n",
    "generate_mel_spectrograms(eval_dataset, \"eval\")\n",
    "generate_mel_spectrograms(val_dataset, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e157e",
   "metadata": {},
   "source": [
    "Caption Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e92f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcf9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_captions(dataset, split_name, max_len=30):\n",
    "    save_dir = f\"features/captions_{split_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    all_count = 0\n",
    "    print(f\"Processing captions for split: {split_name} ...\")\n",
    "\n",
    "    for item in tqdm(dataset):\n",
    "        fname = item[\"fname\"]\n",
    "        captions = item[\"captions\"]  \n",
    "\n",
    "        for i, cap in enumerate(captions):\n",
    "            encoded = tokenizer(\n",
    "                cap.lower(),\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Save encoded caption tensor dict\n",
    "            torch.save(\n",
    "                {\"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "                 \"attention_mask\": encoded[\"attention_mask\"].squeeze(0)},\n",
    "                f\"{save_dir}/{fname}_cap{i}.pt\"\n",
    "            )\n",
    "            all_count += 1\n",
    "\n",
    "    print(f\"✅ Saved {all_count} tokenized captions to {save_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "216809e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing captions for split: train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3839/3839 [00:22<00:00, 171.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 19195 tokenized captions to features/captions_train/\n",
      "Processing captions for split: val ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1045/1045 [00:06<00:00, 160.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 5225 tokenized captions to features/captions_val/\n",
      "Processing captions for split: eval ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1045/1045 [00:07<00:00, 147.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 5225 tokenized captions to features/captions_eval/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_and_save_captions(train_dataset, \"train\")\n",
    "process_and_save_captions(val_dataset, \"val\")\n",
    "process_and_save_captions(eval_dataset, \"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c57ac2",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fbb2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21406b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, mel_dir, caption_dir):\n",
    "        self.mel_dir = mel_dir\n",
    "        self.caption_dir = caption_dir\n",
    "        self.caption_files = sorted(os.listdir(caption_dir))\n",
    "        self.cache = {}  # memory cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cap_file = self.caption_files[idx]\n",
    "        base = cap_file.split(\"_cap\")[0]\n",
    "        mel_path = os.path.join(self.mel_dir, f\"{base}.npy\")\n",
    "\n",
    "        # Cache mel in memory\n",
    "        if base not in self.cache:\n",
    "            mel = np.load(mel_path)\n",
    "            self.cache[base] = torch.tensor(mel, dtype=torch.float32)\n",
    "        mel = self.cache[base]\n",
    "\n",
    "        cap = torch.load(os.path.join(self.caption_dir, cap_file))\n",
    "        return mel, cap[\"input_ids\"], cap[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ec641",
   "metadata": {},
   "source": [
    "Audio Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a7ae7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(256, 256, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.MultiheadAttention(512, num_heads=4, batch_first=True)\n",
    "        self.proj = nn.Linear(512, embed_dim)\n",
    "    \n",
    "    def forward(self, mel):\n",
    "        # mel: [B, 1, 64, T]\n",
    "        x = self.cnn(mel)                    # [B, 256, 8, T/8]\n",
    "        x = x.mean(2).permute(0, 2, 1)       # [B, T', 256]\n",
    "        x, _ = self.rnn(x)\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        pooled = attn_out.mean(1)            # [B, 512]\n",
    "        z = self.proj(pooled)\n",
    "        return F.normalize(z, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b54661",
   "metadata": {},
   "source": [
    "Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "816e2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.proj = nn.Linear(768, embed_dim)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = out.last_hidden_state[:, 0, :]  # CLS token\n",
    "        z = self.proj(cls_emb)\n",
    "        return F.normalize(z, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e7c7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive loss\n",
    "def contrastive_loss(z_a, z_t, temperature=0.07):\n",
    "    sim = z_a @ z_t.T  # cosine similarities\n",
    "    sim /= temperature\n",
    "    labels = torch.arange(sim.size(0)).to(sim.device)\n",
    "    loss_a = F.cross_entropy(sim, labels)\n",
    "    loss_t = F.cross_entropy(sim.T, labels)\n",
    "    return (loss_a + loss_t) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906f2e8",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "795f9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "audio_encoder = AudioEncoder().to(device)\n",
    "text_encoder = TextEncoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c356b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(audio_encoder.parameters()) + list(text_encoder.parameters()), lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e36f9778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch is a list of tuples: (mel, input_ids, attn_mask)\n",
    "    mels, ids, masks = zip(*batch)\n",
    "    \n",
    "    # Find max time length in this batch\n",
    "    max_len = max(m.shape[-1] for m in mels)\n",
    "    \n",
    "    # Pad each mel on the right with zeros\n",
    "    padded_mels = []\n",
    "    for m in mels:\n",
    "        pad_len = max_len - m.shape[-1]\n",
    "        if pad_len > 0:\n",
    "            m = F.pad(m, (0, pad_len))  \n",
    "        padded_mels.append(m)\n",
    "    \n",
    "    mels = torch.stack(padded_mels)\n",
    "    ids = torch.stack(ids)\n",
    "    masks = torch.stack(masks)\n",
    "    return mels, ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94c8734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioTextDataset(\"features/mel_train\", \"features/captions_train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c738c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = AudioTextDataset(\"features/mel_eval\", \"features/captions_eval\")\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84a58961",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 19195\n",
      "[torch.Size([1, 64, 1504]), torch.Size([30]), torch.Size([30])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaira\\AppData\\Local\\Temp\\ipykernel_42904\\1861416668.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cap = torch.load(os.path.join(self.caption_dir, cap_file))\n"
     ]
    }
   ],
   "source": [
    "ds = AudioTextDataset(\"features/mel_train\", \"features/captions_train\")\n",
    "print(\"Dataset length:\", len(ds))\n",
    "x = ds[0]  \n",
    "print([t.shape for t in x if torch.is_tensor(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf1b6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78739394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]C:\\Users\\jaira\\AppData\\Local\\Temp\\ipykernel_42904\\1861416668.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cap = torch.load(os.path.join(self.caption_dir, cap_file))\n",
      "100%|██████████| 1200/1200 [04:13<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1193, Eval loss=2.4171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:08<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss=1.5191, Eval loss=2.3237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:04<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train loss=1.1873, Eval loss=2.3596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:06<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train loss=0.9815, Eval loss=2.3728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:07<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.8343, Eval loss=2.5284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:05<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train loss=0.6746, Eval loss=2.6497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:04<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train loss=0.5727, Eval loss=2.6003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:04<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train loss=0.4714, Eval loss=2.7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:05<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train loss=0.4221, Eval loss=2.7312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:04<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train loss=0.3823, Eval loss=2.7940\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    audio_encoder.train()\n",
    "    text_encoder.train()\n",
    "    train_loss = 0.0\n",
    "    for mel, input_ids, attn_mask in tqdm(train_loader):\n",
    "        mel, input_ids, attn_mask = mel.to(device), input_ids.to(device), attn_mask.to(device)\n",
    "        z_a = audio_encoder(mel)\n",
    "        z_t = text_encoder(input_ids, attn_mask)\n",
    "        loss = contrastive_loss(z_a, z_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    audio_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    eval_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for mel, input_ids, attn_mask in eval_loader:\n",
    "            mel, input_ids, attn_mask = mel.to(device), input_ids.to(device), attn_mask.to(device)\n",
    "            z_a = audio_encoder(mel)\n",
    "            z_t = text_encoder(input_ids, attn_mask)\n",
    "            loss = contrastive_loss(z_a, z_t)\n",
    "            eval_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train loss={train_loss/len(train_loader):.4f}, Eval loss={eval_loss/len(eval_loader):.4f}\")\n",
    "\n",
    "    # Save checkpoint when eval loss improves\n",
    "    if eval_loss < best_loss:\n",
    "        torch.save({\n",
    "            \"audio_encoder\": audio_encoder.state_dict(),\n",
    "            \"text_encoder\": text_encoder.state_dict()\n",
    "        }, \"new_best_model.pt\")\n",
    "        best_loss = eval_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a14c9f",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84dfc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model_a, model_t, dataset, device, batch_size=16):\n",
    "    model_a.eval(); model_t.eval()\n",
    "    audio_embs, text_embs = [], []\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel, input_ids, attn_mask in tqdm(loader, desc=\"Embedding val set\"):\n",
    "            mel, input_ids, attn_mask = mel.to(device), input_ids.to(device), attn_mask.to(device)\n",
    "            z_a = model_a(mel)\n",
    "            z_t = model_t(input_ids, attn_mask)\n",
    "            audio_embs.append(z_a.cpu())\n",
    "            text_embs.append(z_t.cpu())\n",
    "\n",
    "    return torch.cat(audio_embs), torch.cat(text_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63c629c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding val set:   0%|          | 0/327 [00:00<?, ?it/s]C:\\Users\\jaira\\AppData\\Local\\Temp\\ipykernel_42904\\1861416668.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cap = torch.load(os.path.join(self.caption_dir, cap_file))\n",
      "Embedding val set: 100%|██████████| 327/327 [00:23<00:00, 14.01it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = AudioTextDataset(\"features/mel_val\", \"features/captions_val\")\n",
    "audio_embs, text_embs = compute_embeddings(audio_encoder, text_encoder, val_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d04179bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = audio_embs @ text_embs.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adcec7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(sim_matrix, k=10):\n",
    "    correct = 0\n",
    "    for i in range(sim_matrix.size(0)):\n",
    "        topk = sim_matrix[i].topk(k).indices\n",
    "        if i in topk:  # correct caption retrieved\n",
    "            correct += 1\n",
    "    return correct / sim_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d10836b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(sim_matrix):\n",
    "    avg_precisions = []\n",
    "    for i in range(sim_matrix.size(0)):\n",
    "        # Sort by descending similarity\n",
    "        scores = sim_matrix[i].argsort(descending=True)\n",
    "        # Find rank of the correct match\n",
    "        rank = (scores == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "        avg_precisions.append(1.0 / rank)\n",
    "    return sum(avg_precisions) / len(avg_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "316104bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio→Text Recall@1/5/10: 0.008, 0.031, 0.054, MAP=0.028\n",
      "Text→Audio Recall@1/5/10: 0.007, 0.031, 0.055, MAP=0.027\n"
     ]
    }
   ],
   "source": [
    "# Audio → Text\n",
    "r1_a2t = recall_at_k(similarity, 1)\n",
    "r5_a2t = recall_at_k(similarity, 5)\n",
    "r10_a2t = recall_at_k(similarity, 10)\n",
    "map_a2t = mean_average_precision(similarity)\n",
    "\n",
    "# Text → Audio (transpose)\n",
    "r1_t2a = recall_at_k(similarity.T, 1)\n",
    "r5_t2a = recall_at_k(similarity.T, 5)\n",
    "r10_t2a = recall_at_k(similarity.T, 10)\n",
    "map_t2a = mean_average_precision(similarity.T)\n",
    "\n",
    "print(f\"Audio→Text Recall@1/5/10: {r1_a2t:.3f}, {r5_a2t:.3f}, {r10_a2t:.3f}, MAP={map_a2t:.3f}\")\n",
    "print(f\"Text→Audio Recall@1/5/10: {r1_t2a:.3f}, {r5_t2a:.3f}, {r10_t2a:.3f}, MAP={map_t2a:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7fab08",
   "metadata": {},
   "source": [
    "Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e78dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text(audio_path, k=5):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    if wav.dim() == 2:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    mel = mel_transform(wav)\n",
    "    mel_db = to_db(mel).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_a = audio_encoder(mel_db)\n",
    "        z_a = F.normalize(z_a, dim=-1).cpu()   \n",
    "    sims = (z_a @ text_embs.T).squeeze(0)     \n",
    "    topk = sims.topk(k)\n",
    "    return topk.indices.tolist(), topk.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96a5fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Clotho(root=\".\", subset=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3bb13f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10b8bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_dataset)):\n",
    "    sample_val = val_dataset[i]\n",
    "\n",
    "    for caption in sample_val['captions']:\n",
    "        all_val.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47e3e29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10450"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f509a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Caption: Paper rustles as someone walks across the dust and sticks on the ground., score=0.933\n",
      "Rank 2: Caption: A man speaks loudly then a buzzer plays then a man speaks loudly again a flute begins to play., score=0.895\n",
      "Rank 3: Caption: The monotone voice of a man speaks repeatedly followed by a melody played on a keyboard., score=0.890\n",
      "Rank 4: Caption: The rain pours over the houses and dies off after a while., score=0.866\n",
      "Rank 5: Caption: A heavy rain is falling on a windy day., score=0.865\n"
     ]
    }
   ],
   "source": [
    "idxs, scores = audio_to_text(r\"CLOTHO_v2.1\\clotho_audio_files\\validation\\zipping backpack and rustling papers.wav\", k=5)\n",
    "for i, (idx, sc) in enumerate(zip(idxs, scores)):\n",
    "    print(f\"Rank {i+1}: Caption: {all_val[idx]}, score={sc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cc6b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_audio(query_text, k=5):\n",
    "    encoded = tokenizer(query_text, padding=\"max_length\",\n",
    "                        truncation=True, max_length=30, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        z_t = text_encoder(encoded[\"input_ids\"].to(device),\n",
    "                           encoded[\"attention_mask\"].to(device))\n",
    "        z_t = F.normalize(z_t, dim=-1).cpu()   # ensure CPU\n",
    "    sims = (z_t @ audio_embs.T).squeeze(0)\n",
    "    topk = sims.topk(k)\n",
    "    return topk.indices.tolist(), topk.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de4bb2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Audio #5162, score=0.864\n",
      "Rank 2: Audio #5161, score=0.864\n",
      "Rank 3: Audio #5163, score=0.864\n",
      "Rank 4: Audio #5164, score=0.864\n",
      "Rank 5: Audio #5160, score=0.864\n"
     ]
    }
   ],
   "source": [
    "idxs, scores = text_to_audio(\"Sound of wind turbine rotor\", k=5)\n",
    "for i, (idx, sc) in enumerate(zip(idxs, scores)):\n",
    "    print(f\"Rank {i+1}: Audio #{idx}, score={sc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
